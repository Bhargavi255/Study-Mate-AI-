{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "gpuType": "T4",
      "provenance": []
    },
    "accelerator": "GPU",
    "kaggle": {
      "accelerator": "gpu"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -U transformers"
      ],
      "metadata": {
        "id": "ww9cHMzaQhM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Local Inference on GPU\n",
        "Model page: https://huggingface.co/ibm-granite/granite-3.3-2b-instruct\n",
        "\n",
        "âš ï¸ If the generated code snippets do not work, please open an issue on either the [model repo](https://huggingface.co/ibm-granite/granite-3.3-2b-instruct)\n",
        "\t\t\tand/or on [huggingface.js](https://github.com/huggingface/huggingface.js/blob/main/packages/tasks/src/model-libraries-snippets.ts) ðŸ™"
      ],
      "metadata": {
        "id": "YEhQ_cEGQhM8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use a pipeline as a high-level helper\n",
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"text-generation\", model=\"ibm-granite/granite-3.3-2b-instruct\")\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
        "]\n",
        "pipe(messages)"
      ],
      "metadata": {
        "id": "d7JEUigwQhM9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "StudyMate â€” Gradio web application (single-file)\n",
        "Created for hackathon use: attractive UI, PDF upload, semantic search (FAISS), RAG with a chat interface.\n",
        "\n",
        "Features:\n",
        "- Upload one or more PDFs\n",
        "- Clean text extraction with PyMuPDF (fitz)\n",
        "- Chunking with overlap\n",
        "- Embeddings with SentenceTransformers\n",
        "- FAISS vector store\n",
        "- Chat UI (Gradio Blocks): conversation, context viewer, top source highlights, download chat transcript\n",
        "- Two model modes:\n",
        "    1) Local model (transformers AutoModelForCausalLM) â€” if you have enough RAM/GPU\n",
        "    2) Hugging Face Inference API â€” lightweight (needs HF token) â€” recommended for hackathons/demo\n",
        "\n",
        "How to run (recommended for quick hackathon demo):\n",
        "1. Create a virtualenv or use Colab.\n",
        "2. Install dependencies:\n",
        "   pip install gradio==3.36.1 transformers sentence-transformers faiss-cpu pymupdf accelerate safetensors\n",
        "   # If using HF Inference API: pip install requests\n",
        "3. Export HF token if you plan to use the Inference API:\n",
        "   export HUGGINGFACE_TOKEN=\"hf_xxx\"  # or set on Windows via SET\n",
        "4. Run:\n",
        "   python StudyMate_Gradio_App.py\n",
        "\n",
        "Notes for hackathon:\n",
        "- Use HF Inference API to avoid long model loads and GPU requirements.\n",
        "- For local LLM, pick a model you can run on your hardware (e.g., small Llama-family or IBM Granite if available).\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import re\n",
        "import tempfile\n",
        "import time\n",
        "import json\n",
        "from pathlib import Path\n",
        "from typing import List, Tuple\n",
        "\n",
        "import fitz  # PyMuPDF\n",
        "import numpy as np\n",
        "\n",
        "import gradio as gr\n",
        "\n",
        "# Embeddings\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Vector store\n",
        "import faiss\n",
        "\n",
        "# Optional local model (transformers) or HF Inference API\n",
        "try:\n",
        "    from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "    import torch\n",
        "    TRANSFORMERS_AVAILABLE = True\n",
        "except Exception:\n",
        "    TRANSFORMERS_AVAILABLE = False\n",
        "\n",
        "# Optional requests for HF Inference API\n",
        "try:\n",
        "    import requests\n",
        "    REQUESTS_AVAILABLE = True\n",
        "except Exception:\n",
        "    REQUESTS_AVAILABLE = False\n",
        "\n",
        "# ----------------------\n",
        "# Utility functions\n",
        "# ----------------------\n",
        "\n",
        "def extract_text_from_pdf(file_path: str) -> str:\n",
        "    \"\"\"Extract plain text from a PDF file using PyMuPDF (fitz).\"\"\"\n",
        "    doc = fitz.open(file_path)\n",
        "    text = []\n",
        "    for page in doc:\n",
        "        text.append(page.get_text(\"text\"))\n",
        "    return \"\\n\".join(text)\n",
        "\n",
        "\n",
        "def clean_whitespace(text: str) -> str:\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    return text.strip()\n",
        "\n",
        "\n",
        "def chunk_text(text: str, chunk_size: int = 800, overlap: int = 200) -> List[str]:\n",
        "    \"\"\"Split text into overlapping chunks (character-based).\"\"\"\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    L = len(text)\n",
        "    while start < L:\n",
        "        end = min(start + chunk_size, L)\n",
        "        chunk = text[start:end]\n",
        "        chunks.append(chunk.strip())\n",
        "        start += chunk_size - overlap\n",
        "        if start >= L:\n",
        "            break\n",
        "    return chunks\n",
        "\n",
        "\n",
        "# ----------------------\n",
        "# Embedding and FAISS\n",
        "# ----------------------\n",
        "class VectorStore:\n",
        "    def __init__(self, model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\"):\n",
        "        self.embed_model = SentenceTransformer(model_name)\n",
        "        self.index = None\n",
        "        self.metadata = []  # list of dicts {\"text\":..., \"source\":...}\n",
        "        self.dim = None\n",
        "\n",
        "    def build(self, texts: List[str], metadatas: List[dict]):\n",
        "        enc = self.embed_model.encode(texts, convert_to_numpy=True, show_progress_bar=True)\n",
        "        self.dim = enc.shape[1]\n",
        "        self.index = faiss.IndexFlatL2(self.dim)\n",
        "        self.index.add(enc)\n",
        "        self.metadata = metadatas\n",
        "\n",
        "    def add(self, texts: List[str], metadatas: List[dict]):\n",
        "        enc = self.embed_model.encode(texts, convert_to_numpy=True, show_progress_bar=False)\n",
        "        if self.index is None:\n",
        "            self.dim = enc.shape[1]\n",
        "            self.index = faiss.IndexFlatL2(self.dim)\n",
        "        self.index.add(enc)\n",
        "        self.metadata.extend(metadatas)\n",
        "\n",
        "    def search(self, query: str, k: int = 4) -> List[Tuple[dict, float]]:\n",
        "        q = self.embed_model.encode([query], convert_to_numpy=True)\n",
        "        if self.index is None:\n",
        "            return []\n",
        "        distances, indices = self.index.search(q, k)\n",
        "        results = []\n",
        "        for idx, dist in zip(indices[0], distances[0]):\n",
        "            if idx < 0 or idx >= len(self.metadata):\n",
        "                continue\n",
        "            results.append((self.metadata[idx], float(dist)))\n",
        "        return results\n",
        "\n",
        "\n",
        "# ----------------------\n",
        "# LLM wrapper\n",
        "# ----------------------\n",
        "class LLM:\n",
        "    def __init__(self, mode: str = \"hf_api\", hf_model: str = None, hf_token: str = None, local_model: str = None):\n",
        "        \"\"\"\n",
        "        mode: 'hf_api' (Hugging Face Inference API) or 'local' (transformers)\n",
        "        hf_model: model id for HF inference API\n",
        "        local_model: model id for AutoModelForCausalLM\n",
        "        \"\"\"\n",
        "        self.mode = mode\n",
        "        self.hf_model = hf_model\n",
        "        self.hf_token = hf_token or os.environ.get(\"HUGGINGFACE_TOKEN\")\n",
        "        self.local_model = local_model\n",
        "        self._loaded = False\n",
        "        self.tokenizer = None\n",
        "        self.model = None\n",
        "        if self.mode == \"local\" and not TRANSFORMERS_AVAILABLE:\n",
        "            raise RuntimeError(\"transformers not available for local mode\")\n",
        "\n",
        "    def load_local(self):\n",
        "        if self._loaded:\n",
        "            return\n",
        "        assert self.local_model is not None\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.local_model)\n",
        "        # load with low_cpu_mem_usage in recent transformers\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(self.local_model, device_map=\"auto\")\n",
        "        self._loaded = True\n",
        "\n",
        "    def generate(self, prompt: str, max_tokens: int = 256) -> str:\n",
        "        if self.mode == \"hf_api\":\n",
        "            if not REQUESTS_AVAILABLE:\n",
        "                return \"ERROR: requests not installed. Install requests to use HF Inference API.\"\n",
        "            assert self.hf_model is not None, \"HF model id required for hf_api mode\"\n",
        "            assert self.hf_token is not None, \"HUGGINGFACE_TOKEN missing for hf_api mode\"\n",
        "            api_url = f\"https://api-inference.huggingface.co/models/{self.hf_model}\"\n",
        "            headers = {\"Authorization\": f\"Bearer {self.hf_token}\"}\n",
        "            payload = {\"inputs\": prompt, \"parameters\": {\"max_new_tokens\": max_tokens}}\n",
        "            resp = requests.post(api_url, headers=headers, json=payload)\n",
        "            if resp.status_code != 200:\n",
        "                return f\"Inference API error {resp.status_code}: {resp.text}\"\n",
        "            out = resp.json()\n",
        "            if isinstance(out, dict) and out.get(\"error\"):\n",
        "                return f\"Inference API error: {out.get('error')}\"\n",
        "            # HF Inference API sometimes returns a list of dicts\n",
        "            if isinstance(out, list) and len(out) > 0 and \"generated_text\" in out[0]:\n",
        "                return out[0][\"generated_text\"]\n",
        "            if isinstance(out, dict) and \"generated_text\" in out:\n",
        "                return out[\"generated_text\"]\n",
        "            return str(out)\n",
        "        else:\n",
        "            # local generation\n",
        "            if not self._loaded:\n",
        "                self.load_local()\n",
        "            inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model.generate(**inputs, max_new_tokens=max_tokens)\n",
        "            decoded = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "            # remove prompt prefix\n",
        "            return decoded[len(prompt):].strip()\n",
        "\n",
        "\n",
        "# ----------------------\n",
        "# Application state\n",
        "# ----------------------\n",
        "STATE = {\n",
        "    \"vectorstore\": None,\n",
        "    \"llm\": None,\n",
        "    \"chunks\": [],\n",
        "    \"metadatas\": [],\n",
        "}\n",
        "\n",
        "# ----------------------\n",
        "# Core pipeline actions\n",
        "# ----------------------\n",
        "\n",
        "def ingest_pdfs(files, chunk_size=800, overlap=200):\n",
        "    \"\"\"Extract, chunk, and build vector store. Returns summary info.\"\"\"\n",
        "    all_texts = []\n",
        "    metadatas = []\n",
        "    for f in files:\n",
        "        fname = f.name if hasattr(f, \"name\") else f\n",
        "        # save to temp and extract\n",
        "        if hasattr(f, \"name\"):\n",
        "            tmp_path = f.name\n",
        "        else:\n",
        "            tmp_path = str(f)\n",
        "        text = extract_text_from_pdf(tmp_path)\n",
        "        text = clean_whitespace(text)\n",
        "        chunks = chunk_text(text, chunk_size=chunk_size, overlap=overlap)\n",
        "        for i, c in enumerate(chunks):\n",
        "            all_texts.append(c)\n",
        "            metadatas.append({\"source\": os.path.basename(tmp_path), \"chunk_id\": i, \"text\": c[:500]})\n",
        "\n",
        "    vs = VectorStore()\n",
        "    vs.build(all_texts, metadatas)\n",
        "    STATE[\"vectorstore\"] = vs\n",
        "    STATE[\"chunks\"] = all_texts\n",
        "    STATE[\"metadatas\"] = metadatas\n",
        "    return {\"n_chunks\": len(all_texts), \"n_files\": len(files)}\n",
        "\n",
        "\n",
        "def answer_question(question: str, k: int = 4, llm_mode: str = \"hf_api\", hf_model: str = None, hf_token: str = None, local_model: str = None) -> Tuple[str, List[dict]]:\n",
        "    \"\"\"Search vectorstore, build context, and call LLM. Returns (answer, sources).\"\"\"\n",
        "    vs: VectorStore = STATE.get(\"vectorstore\")\n",
        "    if vs is None:\n",
        "        return \"Please upload PDFs and build the knowledge base first.\", []\n",
        "\n",
        "    results = vs.search(question, k=k)\n",
        "    # aggregate context\n",
        "    context_pieces = []\n",
        "    sources = []\n",
        "    for meta, dist in results:\n",
        "        context_pieces.append(meta[\"text\"])\n",
        "        sources.append({\"source\": meta[\"source\"], \"chunk_id\": meta[\"chunk_id\"], \"dist\": dist, \"snippet\": meta[\"text\"]})\n",
        "\n",
        "    context = \"\\n\\n---\\n\\n\".join(context_pieces)\n",
        "\n",
        "    prompt = (\n",
        "        \"You are StudyMate, an AI academic assistant. Use the provided context from the user's uploaded PDFs to answer the question. If the answer is not contained in the context, say you don't know and provide suggestions to locate the answer.\"\n",
        "        f\"\\n\\nContext:\\n{context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\n",
        "    )\n",
        "\n",
        "    # initialize LLM wrapper if necessary\n",
        "    if STATE.get(\"llm\") is None or STATE[\"llm\"].mode != llm_mode:\n",
        "        STATE[\"llm\"] = LLM(mode=llm_mode, hf_model=hf_model, hf_token=hf_token, local_model=local_model)\n",
        "\n",
        "    llm = STATE[\"llm\"]\n",
        "    answer = llm.generate(prompt, max_tokens=256)\n",
        "    return answer, sources\n",
        "\n",
        "\n",
        "# ----------------------\n",
        "# Gradio UI\n",
        "# ----------------------\n",
        "\n",
        "css = \"\"\"\n",
        ":root{\n",
        "  --primary:#7c3aed; /* violet-600 */\n",
        "  --accent:#06b6d4;  /* cyan-500 */\n",
        "}\n",
        ".gradio-container {background: linear-gradient(180deg, rgba(124,58,237,0.06), rgba(6,182,212,0.03));}\n",
        ".header{padding:18px;border-radius:14px;margin-bottom:12px}\n",
        ".card{background:white;padding:14px;border-radius:12px;box-shadow:0 6px 24px rgba(15,23,42,0.06)}\n",
        ".small-muted{font-size:12px;color:#6b7280}\n",
        ".source-bubble{background:rgba(15,23,42,0.04);padding:10px;border-radius:8px;margin:6px 0}\n",
        "\"\"\"\n",
        "\n",
        "with gr.Blocks(css=css, title=\"StudyMate â€” Hackathon-ready\", theme=gr.themes.Soft()) as app:\n",
        "    with gr.Row(elem_id=\"top-row\"):\n",
        "        with gr.Column(scale=1):\n",
        "            gr.Markdown(\"# ðŸŽ“ StudyMate â€” Conversational PDF Q&A\")\n",
        "            gr.Markdown(\"Upload PDFs, then ask questions. Designed for hackathons: fast demo mode via Hugging Face Inference API or local model option.\")\n",
        "            with gr.Accordion(\"How it works\", open=False):\n",
        "                gr.Markdown(\n",
        "                    \"1. Upload PDFs â†’ 2. We extract & chunk text â†’ 3. Build embeddings & FAISS index â†’ 4. Ask questions (RAG) using an LLM.\"\n",
        "                )\n",
        "\n",
        "        with gr.Column(scale=1):\n",
        "            # Controls card\n",
        "            with gr.Column(elem_classes=\"card header\"):\n",
        "                hf_token_input = gr.Textbox(label=\"Hugging Face Token (optional for HF Inference API)\", placeholder=\"hf_xxx (recommended for quick demos)\", type=\"password\")\n",
        "                hf_model_input = gr.Textbox(label=\"HF model (inference API) [default: 'gpt2']\", value=\"gpt2\")\n",
        "                local_model_input = gr.Textbox(label=\"Local model ID (transformers) - optional\", placeholder=\"e.g. ibm-granite/granite-3.3-2b-instruct\")\n",
        "                llm_mode_radio = gr.Radio(label=\"LLM Mode\", choices=[\"hf_api\", \"local\"], value=\"hf_api\")\n",
        "                chunk_size_slider = gr.Slider(label=\"Chunk size (chars)\", minimum=300, maximum=2000, step=50, value=800)\n",
        "                overlap_slider = gr.Slider(label=\"Chunk overlap (chars)\", minimum=50, maximum=800, step=25, value=200)\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "            upload = gr.File(label=\"Upload PDF(s)\", file_count=\"multiple\", file_types=[\".pdf\"])\n",
        "            ingest_btn = gr.Button(\"Build Knowledge Base\", elem_id=\"ingest_btn\")\n",
        "            ingest_output = gr.Textbox(label=\"Ingest status\", interactive=False)\n",
        "\n",
        "            gr.Markdown(\"---\")\n",
        "            gr.Markdown(\"### Conversation\")\n",
        "            chatbot = gr.Chatbot(label=\"StudyMate Chat\", height=380)\n",
        "            user_input = gr.Textbox(label=\"Your question\", placeholder=\"Ask anything about the uploaded PDFs...\")\n",
        "            with gr.Row():\n",
        "                ask_btn = gr.Button(\"Ask\")\n",
        "                clear_btn = gr.Button(\"Clear chat\")\n",
        "\n",
        "        with gr.Column(scale=0.7):\n",
        "            gr.Markdown(\"### Top Sources\")\n",
        "            sources_box = gr.Markdown(\"Upload PDFs and build the KB to see top sources here.\")\n",
        "            gr.Markdown(\"---\")\n",
        "            gr.Markdown(\"### Settings & Export\")\n",
        "            k_slider = gr.Slider(label=\"# retrieved chunks (k)\", minimum=1, maximum=10, step=1, value=4)\n",
        "            download_btn = gr.Button(\"Download Transcript\")\n",
        "\n",
        "    # Hidden state\n",
        "    transcript_store = gr.State([])\n",
        "\n",
        "    def on_ingest(files, chunk_size, overlap, hf_token_field):\n",
        "        if not files:\n",
        "            return \"No files uploaded.\" , None\n",
        "        # gradio File gives temp filenames; just pass them\n",
        "        files_list = files\n",
        "        info = ingest_pdfs(files_list, chunk_size=int(chunk_size), overlap=int(overlap))\n",
        "        msg = f\"Indexed {info['n_chunks']} chunks from {info['n_files']} files. Ready to ask questions.\"\n",
        "        if hf_token_field:\n",
        "            os.environ[\"HUGGINGFACE_TOKEN\"] = hf_token_field\n",
        "        return msg, info\n",
        "\n",
        "    ingest_btn.click(on_ingest, inputs=[upload, chunk_size_slider, overlap_slider, hf_token_input], outputs=[ingest_output, ingest_output])\n",
        "\n",
        "    def on_ask(question, k, llm_mode, hf_model, hf_token, local_model, chat_history, transcript):\n",
        "        if not question or question.strip()==\"\":\n",
        "            return chat_history, transcript\n",
        "        # call answer pipeline\n",
        "        answer, sources = answer_question(question, k=int(k), llm_mode=llm_mode, hf_model=hf_model, hf_token=hf_token, local_model=local_model)\n",
        "        # append to chat\n",
        "        chat_history = chat_history + [(\"You\", question), (\"StudyMate\", answer)]\n",
        "        # update transcript\n",
        "        transcript = transcript + [{\"q\":question, \"a\":answer, \"sources\":sources}]\n",
        "        # update sources box text\n",
        "        sources_md = \"### Top Sources\\n\"\n",
        "        for s in sources:\n",
        "            sources_md += f\"- **{s['source']}** (dist={s['dist']:.3f}) â€” {s['snippet'][:200]}...\\n\"\n",
        "        return chat_history, transcript, sources_md\n",
        "\n",
        "    ask_btn.click(on_ask, inputs=[user_input, k_slider, llm_mode_radio, hf_model_input, hf_token_input, local_model_input, chatbot, transcript_store], outputs=[chatbot, transcript_store, sources_box])\n",
        "\n",
        "    def clear_chat():\n",
        "        return [], []\n",
        "    clear_btn.click(clear_chat, outputs=[chatbot, transcript_store])\n",
        "\n",
        "    def download_transcript(transcript):\n",
        "        if not transcript:\n",
        "            return None\n",
        "        fname = f\"transcript_{int(time.time())}.json\"\n",
        "        with open(fname, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(transcript, f, ensure_ascii=False, indent=2)\n",
        "        return gr.File.update(value=fname, visible=True)\n",
        "\n",
        "    download_btn.click(download_transcript, inputs=[transcript_store], outputs=[gr.File()])\n",
        "\n",
        "    # Run\n",
        "    demo = app\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch(server_name=\"0.0.0.0\", server_port=7860, share=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "id": "j_cX8Y-jQhM-",
        "outputId": "fec9b7e1-c626-4cd1-9cb3-d425fffce2c9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2621103597.py:313: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot = gr.Chatbot(label=\"StudyMate Chat\", height=380)\n",
            "/usr/local/lib/python3.12/dist-packages/gradio/layouts/column.py:59: UserWarning: 'scale' value should be an integer. Using 0.7 will cause issues.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://12df33bc52d9f6d2bb.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://12df33bc52d9f6d2bb.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T-bRdx0gQoMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d7YiWjf9QoFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MIdtC-uLScrY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}